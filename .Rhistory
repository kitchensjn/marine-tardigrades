var(travelInsurance$Duration)
#Destination
getmode(travelInsurance$Destination)
#Net Sales
range(travelInsurance$Net.Sales)
getmode(travelInsurance$Net.Sales)
mean(travelInsurance$Net.Sales)
median(travelInsurance$Net.Sales)
var(travelInsurance$Net.Sales)
#Commission
range(travelInsurance$Commision..in.value.)
getmode(travelInsurance$Commision..in.value.)
mean(travelInsurance$Commision..in.value.)
median(travelInsurance$Commision..in.value.)
var(travelInsurance$Commision..in.value.)
#Gender
getmode(travelInsurance$Gender)
#Age
range(travelInsurance$Age)
getmode(travelInsurance$Age)
mean(travelInsurance$Age)
median(travelInsurance$Age)
var(travelInsurance$Age)
## Clean Data ##
# Remove rows with empty values
travel_cleaned1 <- na.omit(travelInsurance)
travel_cleaned1 <- travelInsurance[-which(travelInsurance$Gender==""),]
cat("The dataset used to have ", nrow(travelInsurance), "records and after cleaning it it has ", nrow(travel_cleaned1), "records")
#this causes more than half of the individuals to be removed, so we will not consider gender in our analysis, and keep all entries
#remove entries where Duration < 0 or Age > 112
travel_cleaned<-travelInsurance[-which(travelInsurance$Duration<0 | travelInsurance$Age>112),]
cat("The dataset used to have ", nrow(travelInsurance), "records and after cleaning it it has ", nrow(travel_cleaned), "records")
#The dataset used to have  63326 records and after cleaning it it has  62342 records
#select only key attributes
#remove Gender column, Agency, Product Name and Destination column
# too many destinations increases the number of features when performing dummy enconding and slows the process
travel_cleaned<-subset(travel_cleaned,select=-c(Gender, Destination,Agency, Product.Name))
#only remove outliers of important attribute
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Duration)
#removed 5089 entries
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Net.Sales)
#removed 2475 entries
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Age)
#removed 0 entries
##simple statistics on cleaned data
##visualizations
#change Claim to binary no=0, yes=1
travel_cleaned<-travel_cleaned %>%
mutate(Claim=ifelse(Claim=="No",0,1))
#convert Agency.Type to binary Airlines = 0, Travel Agency = 1
travel_cleaned<-travel_cleaned %>%
mutate(Agency.Type=ifelse(Agency.Type=="Airlines",0,1))
#convert Distribution.Channel to binary Offline = 0, Online=1
travel_cleaned<-travel_cleaned %>%
mutate(Distribution.Channel=ifelse(Distribution.Channel=="Offline",0,1))
#normalize the data
travel_cleaned$Duration <- normalize(travel_cleaned$Duration)
travel_cleaned$Net.Sales <- normalize(travel_cleaned$Net.Sales)
travel_cleaned$Commision..in.value. <- normalize(travel_cleaned$Commision..in.value.)
travel_cleaned$Age <- normalize(travel_cleaned$Age)
#dummy variables
#travel_encoded <- dummy_cols(travel_cleaned, remove_first_dummy = TRUE)
#head(travel_encoded)
### PART 2-PREDICT ClAIM STATUS ###
##TRAINING AND TESTING SETS
set.seed(1)
numTrainSample <- round(nrow(travel_cleaned) * 0.60)
trainIdx <- sample(nrow(travel_cleaned), numTrainSample)
train <- travel_cleaned[trainIdx, ]
test <- travel_cleaned[-trainIdx, ]
# LOGISTIC MODEL
#log_reg = LogisticRegression(class_weight = 'balanced')
m <- glm(Claim ~ Agency.Type + Net.Sales + Age  + Commision..in.value.,
data = train,
family = "binomial") # binomial corresponds logistic reg.
summary(m)
# prediction
predict(m, test)
prob <- predict(m, test, type = "response")
summary(prob)
#ROCR curve
data(ROCR.simple)
df <- data.frame(ROCR.simple)
pred <- prediction(df$predictions, df$labels)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
pred <- rep(0, nrow(test))
pred[prob > 0.2] = 1
#Confusion Matrix
table(test$Claim, pred)
#      0
#0 19891
#1   230
#Miscalculation Rate is %11.43
mean(pred != test$Claim)
##UNDERSAMPLING
travel_undersampling <- ovun.sample(Claim~. , data= travel_cleaned, method = "under",
p=0.5, seed = 1)$data
table(travel_undersampling$Claim)
numTrainSample_under <- round(nrow(travel_undersampling) * 0.60)
trainIdx_under <- sample(nrow(travel_undersampling), numTrainSample_under)
train_under <- travel_undersampling[trainIdx_under, ]
test_under <- travel_undersampling[-trainIdx_under, ]
table(train_under$Claim)
table(test_under$Claim)
m_under <- glm(Claim ~ Agency.Type + Net.Sales + Age  + Commision..in.value.,
data = train_under,
family = "binomial") # binomial corresponds logistic reg.
summary(m_under)
# prediction
predict(m_under, test_under)
prob_under <- predict(m_under, test_under, type = "response")
summary(prob_under)
pred_under <- rep(0, nrow(test_under))
pred_under[prob > 0.2] = 1
#Confusion Matrix
table(test_under$Claim, pred_under)
travel_undersampling <- ovun.sample(Claim~. , data= travel_cleaned, method = "under",
p=0.5, seed = 1)$data
table(travel_undersampling$Claim)
numTrainSample_under <- round(nrow(travel_undersampling) * 0.60)
trainIdx_under <- sample(nrow(travel_undersampling), numTrainSample_under)
train_under <- travel_undersampling[trainIdx_under, ]
test_under <- travel_undersampling[-trainIdx_under, ]
table(train_under$Claim)
table(test_under$Claim)
m_under <- glm(Claim ~ Agency.Type + Net.Sales + Age  + Commision..in.value.,
data = train_under,
family = "binomial") # binomial corresponds logistic reg.
summary(m_under)
# prediction
predict(m_under, test_under)
prob_under <- predict(m_under, test_under, type = "response")
summary(prob_under)
pred_under <- rep(0, nrow(test_under))
pred_under[prob_under > 0.2] = 1
#Confusion Matrix
table(test_under$Claim, pred_under)
predict(m_under, test_under)
prob_under <- predict(m_under, test_under, type = "response")
summary(prob_under)
pred_under <- rep(0, nrow(test_under))
pred_under[prob_under > 0.5] = 1
#Confusion Matrix
table(test_under$Claim, pred_under)
travel_oversampling <- ovun.sample(Claim~. , data= travel_cleaned, method = "over",
p=0.5, seed = 1)$data
table(travel_oversampling$Claim)
numTrainSample_over <- round(nrow(travel_oversampling) * 0.60)
trainIdx_over <- sample(nrow(travel_oversampling), numTrainSample_over)
train_over <- travel_oversampling[trainIdx_over, ]
test_over <- travel_oversampling[-trainIdx_over, ]
table(train_over$Claim)
table(test_over$Claim)
m_over <- glm(Claim ~ Agency.Type + Net.Sales + Age  + Commision..in.value.,
data = train_over,
family = "binomial") # binomial corresponds logistic reg.
summary(m_over)
# prediction
predict(m_over, test_over)
prob_over <- predict(m_over, test_over, type = "response")
summary(prob_over)
pred_over <- rep(0, nrow(test_over))
pred_over[prob_over > 0.5] = 1
#Confusion Matrix
table(test_over$Claim, pred_over)
data(ROCR.simple)
df <- data.frame(ROCR.simple)
pred <- prediction(df$predictions, df$labels)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
View(df)
#library(fastDummies)
library(dplyr)
#library(spatialEco)
library(ROCR)
library(ggplot2)
library(gplots)
library(ROSE)
## Import Dataset
travelInsurance <- read.csv("/Users/jameskitchens/Downloads/travel_insurance.csv")
head(travelInsurance)
## Necessary Functions
#mode function
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
#normalize function
normalize <- function(x) {
cat ("Normalizing data, keep in mind  that the min and max values are:", min(x), " and ", max(x) )
return ((x - min(x)) / (max(x) - min(x)))
}
#remove outliers
removes_outliers <- function(df, x) {
# Adds z score to price feature
df$z_score <- c(outliers(x))
# Show number of extreme outliers using Z-score
cat("Removing the following outliers:", nrow(df[which(df$z_score >= 3 | df$z_score <= -3),]))
# Removes values where z-score >
df <- df[which(df$z_score < 3 & df$z_score > -3),]
df$z_score <- NULL
return(df)
}
##Simple Statistics##
#Agency
getmode(travelInsurance$Agency)
#Agency.Type
getmode(travelInsurance$Agency.Type)
#Distribution.Channel
getmode(travelInsurance$Distribution.Channel)
#Product.Name
getmode(travelInsurance$Product.Name)
#Claim
getmode(travelInsurance$Claim)
#Duration
range(travelInsurance$Duration)
getmode(travelInsurance$Duration)
mean(travelInsurance$Duration)
median(travelInsurance$Duration)
var(travelInsurance$Duration)
#Destination
getmode(travelInsurance$Destination)
#Net Sales
range(travelInsurance$Net.Sales)
getmode(travelInsurance$Net.Sales)
mean(travelInsurance$Net.Sales)
median(travelInsurance$Net.Sales)
var(travelInsurance$Net.Sales)
#Commission
range(travelInsurance$Commision..in.value.)
getmode(travelInsurance$Commision..in.value.)
mean(travelInsurance$Commision..in.value.)
median(travelInsurance$Commision..in.value.)
var(travelInsurance$Commision..in.value.)
#Gender
getmode(travelInsurance$Gender)
#Age
range(travelInsurance$Age)
getmode(travelInsurance$Age)
mean(travelInsurance$Age)
median(travelInsurance$Age)
var(travelInsurance$Age)
## Clean Data ##
# Remove rows with empty values
travel_cleaned1 <- na.omit(travelInsurance)
travel_cleaned1 <- travelInsurance[-which(travelInsurance$Gender==""),]
cat("The dataset used to have ", nrow(travelInsurance), "records and after cleaning it it has ", nrow(travel_cleaned1), "records")
#this causes more than half of the individuals to be removed, so we will not consider gender in our analysis, and keep all entries
#remove entries where Duration < 0 or Age > 112
travel_cleaned<-travelInsurance[-which(travelInsurance$Duration<0 | travelInsurance$Age>112),]
cat("The dataset used to have ", nrow(travelInsurance), "records and after cleaning it it has ", nrow(travel_cleaned), "records")
#The dataset used to have  63326 records and after cleaning it it has  62342 records
#select only key attributes
#remove Gender column, Agency, Product Name and Destination column
# too many destinations increases the number of features when performing dummy enconding and slows the process
travel_cleaned<-subset(travel_cleaned,select=-c(Gender, Destination,Agency, Product.Name))
#only remove outliers of important attribute
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Duration)
#removed 5089 entries
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Net.Sales)
#removed 2475 entries
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Age)
#removed 0 entries
##simple statistics on cleaned data
##visualizations
#change Claim to binary no=0, yes=1
travel_cleaned<-travel_cleaned %>%
mutate(Claim=ifelse(Claim=="No",0,1))
#convert Agency.Type to binary Airlines = 0, Travel Agency = 1
travel_cleaned<-travel_cleaned %>%
mutate(Agency.Type=ifelse(Agency.Type=="Airlines",0,1))
#convert Distribution.Channel to binary Offline = 0, Online=1
travel_cleaned<-travel_cleaned %>%
mutate(Distribution.Channel=ifelse(Distribution.Channel=="Offline",0,1))
#normalize the data
travel_cleaned$Duration <- normalize(travel_cleaned$Duration)
travel_cleaned$Net.Sales <- normalize(travel_cleaned$Net.Sales)
travel_cleaned$Commision..in.value. <- normalize(travel_cleaned$Commision..in.value.)
travel_cleaned$Age <- normalize(travel_cleaned$Age)
#dummy variables
#travel_encoded <- dummy_cols(travel_cleaned, remove_first_dummy = TRUE)
#head(travel_encoded)
### PART 2-PREDICT ClAIM STATUS ###
##TRAINING AND TESTING SETS
set.seed(1)
numTrainSample <- round(nrow(travel_cleaned) * 0.60)
trainIdx <- sample(nrow(travel_cleaned), numTrainSample)
train <- travel_cleaned[trainIdx, ]
test <- travel_cleaned[-trainIdx, ]
# LOGISTIC MODEL
#log_reg = LogisticRegression(class_weight = 'balanced')
m <- glm(Claim ~ Agency.Type + Net.Sales + Age  + Commision..in.value.,
data = train,
family = "binomial") # binomial corresponds logistic reg.
summary(m)
# prediction
predict(m, test)
prob <- predict(m, test, type = "response")
summary(prob)
#library(fastDummies)
library(dplyr)
#library(spatialEco)
library(ROCR)
library(ggplot2)
library(gplots)
library(ROSE)
## Import Dataset
travelInsurance <- read.csv("/Users/jameskitchens/Downloads/travel_insurance.csv")
head(travelInsurance)
## Necessary Functions
#mode function
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
#normalize function
normalize <- function(x) {
cat ("Normalizing data, keep in mind  that the min and max values are:", min(x), " and ", max(x) )
return ((x - min(x)) / (max(x) - min(x)))
}
#remove outliers
removes_outliers <- function(df, x) {
# Adds z score to price feature
df$z_score <- c(outliers(x))
# Show number of extreme outliers using Z-score
cat("Removing the following outliers:", nrow(df[which(df$z_score >= 3 | df$z_score <= -3),]))
# Removes values where z-score >
df <- df[which(df$z_score < 3 & df$z_score > -3),]
df$z_score <- NULL
return(df)
}
##Simple Statistics##
#Agency
getmode(travelInsurance$Agency)
#Agency.Type
getmode(travelInsurance$Agency.Type)
#Distribution.Channel
getmode(travelInsurance$Distribution.Channel)
#Product.Name
getmode(travelInsurance$Product.Name)
#Claim
getmode(travelInsurance$Claim)
#Duration
range(travelInsurance$Duration)
getmode(travelInsurance$Duration)
mean(travelInsurance$Duration)
median(travelInsurance$Duration)
var(travelInsurance$Duration)
#Destination
getmode(travelInsurance$Destination)
#Net Sales
range(travelInsurance$Net.Sales)
getmode(travelInsurance$Net.Sales)
mean(travelInsurance$Net.Sales)
median(travelInsurance$Net.Sales)
var(travelInsurance$Net.Sales)
#Commission
range(travelInsurance$Commision..in.value.)
getmode(travelInsurance$Commision..in.value.)
mean(travelInsurance$Commision..in.value.)
median(travelInsurance$Commision..in.value.)
var(travelInsurance$Commision..in.value.)
#Gender
getmode(travelInsurance$Gender)
#Age
range(travelInsurance$Age)
getmode(travelInsurance$Age)
mean(travelInsurance$Age)
median(travelInsurance$Age)
var(travelInsurance$Age)
## Clean Data ##
# Remove rows with empty values
travel_cleaned1 <- na.omit(travelInsurance)
travel_cleaned1 <- travelInsurance[-which(travelInsurance$Gender==""),]
cat("The dataset used to have ", nrow(travelInsurance), "records and after cleaning it it has ", nrow(travel_cleaned1), "records")
#this causes more than half of the individuals to be removed, so we will not consider gender in our analysis, and keep all entries
#remove entries where Duration < 0 or Age > 112
travel_cleaned<-travelInsurance[-which(travelInsurance$Duration<0 | travelInsurance$Age>112),]
cat("The dataset used to have ", nrow(travelInsurance), "records and after cleaning it it has ", nrow(travel_cleaned), "records")
#The dataset used to have  63326 records and after cleaning it it has  62342 records
#select only key attributes
#remove Gender column, Agency, Product Name and Destination column
# too many destinations increases the number of features when performing dummy enconding and slows the process
travel_cleaned<-subset(travel_cleaned,select=-c(Gender, Destination,Agency, Product.Name))
#only remove outliers of important attribute
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Duration)
#removed 5089 entries
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Net.Sales)
#removed 2475 entries
#travel_cleaned<- removes_outliers(travel_cleaned, travel_cleaned$Age)
#removed 0 entries
##simple statistics on cleaned data
##visualizations
#change Claim to binary no=0, yes=1
travel_cleaned<-travel_cleaned %>%
mutate(Claim=ifelse(Claim=="No",0,1))
#convert Agency.Type to binary Airlines = 0, Travel Agency = 1
travel_cleaned<-travel_cleaned %>%
mutate(Agency.Type=ifelse(Agency.Type=="Airlines",0,1))
#convert Distribution.Channel to binary Offline = 0, Online=1
travel_cleaned<-travel_cleaned %>%
mutate(Distribution.Channel=ifelse(Distribution.Channel=="Offline",0,1))
#normalize the data
travel_cleaned$Duration <- normalize(travel_cleaned$Duration)
travel_cleaned$Net.Sales <- normalize(travel_cleaned$Net.Sales)
travel_cleaned$Commision..in.value. <- normalize(travel_cleaned$Commision..in.value.)
travel_cleaned$Age <- normalize(travel_cleaned$Age)
#dummy variables
#travel_encoded <- dummy_cols(travel_cleaned, remove_first_dummy = TRUE)
#head(travel_encoded)
### PART 2-PREDICT ClAIM STATUS ###
##TRAINING AND TESTING SETS
set.seed(1)
numTrainSample <- round(nrow(travel_cleaned) * 0.60)
trainIdx <- sample(nrow(travel_cleaned), numTrainSample)
train <- travel_cleaned[trainIdx, ]
test <- travel_cleaned[-trainIdx, ]
# LOGISTIC MODEL
#log_reg = LogisticRegression(class_weight = 'balanced')
m <- glm(Claim ~ Agency.Type + Net.Sales + Age  + Commision..in.value.,
data = train,
family = "binomial") # binomial corresponds logistic reg.
summary(m)
# prediction
predict(m, test)
prob <- predict(m, test, type = "response")
summary(prob)
#ROCR curve
pred <- prediction(prob, test$Claim)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
install.packages(shiny)
install.packages("shiny")
install.packages("shiny")
install.packages("DT")
install.packages("RColorBrewer")
install.packages("remotes")
remotes::install_github("rstudio/reticulate", force = T, ref = '967a9a750e2f2e8ca61a9fe9fc3616bc63f97399')
devtools::install_github('andrewsali/shinycssloaders')
install.packages(devtools)
install.packages("devtools")
devtools::install_github('andrewsali/shinycssloaders')
install.packages("reticulate")
reticulate::virtualenv_list()
reticulate::conda_list()
reticulate::virtualenv_create(envname = 'python35_env', python= '/usr/bin/python3')
setwd("~/Documents/GitHub/2020Spring_JPL_CentralValleyWater/Data/Data_Output_From_Code/NGWMN_Subbasin_Measurement_Norm_GRACE_20200303_152026911167")
file <- read.csv("NGWMN_Subbasin_Measurement_Norm_GRACE_20200303_152026911167.csv")
file$MSMT_DATE <- as.Date(file$MSMT_DATE)
file <- file[which(file$MSMT_DATE > "2010-01-01"),]
View(file)
file <- file[-which(is.na(file$GRACE_VALUE),]
file <- file[-which(is.na(file$GRACE_VALUE)),]
View(file)
ggplot(data=file) +
geom_point(aes(WaterLevelInFeetRelativeToNGVD29_NORM, GRACE_VALUE))
library(ggplot2)
ggplot(data=file) +
geom_point(aes(WaterLevelInFeetRelativeToNGVD29_NORM, GRACE_VALUE))
setwd("~/Documents/GitHub/2020Spring_JPL_CentralValleyWater/Data/Data_Output_From_Code/NGWMN_Subbasin_Measurement_Norm_GRACE_Norm_20200304_161523719380")
wellAndGrace <- read.csv("NGWMN_Subbasin_Measurement_Norm_GRACE_Norm_20200304_161523719380.csv")
library(ggplot2)
wellAndGrace <- read.csv("NGWMN_Subbasin_Measurement_Norm_GRACE_Norm_20200304_161523719380.csv")
wellAndGrace <- wellAndGrace[-which(is.na(wellAndGrace$GRACE_VALUE_NORM)),]
linearModel <- lm(GRACE_VALUE_NORM~WaterLevelInFeetRelativeToNGVD29_NORM, data=wellAndGrace)
summary(linearModel)
wellAndGrace$MSMT_DATE <- as.Date(wellAndGrace$MSMT_DATE, format="%Y-%m-%d")
j <- data.frame(table(wellAndGrace$SiteName))
ggplot(data=wellAndGrace) +
geom_point(aes(WaterLevelInFeetRelativeToNGVD29_NORM, GRACE_VALUE_NORM, color=SiteName))
j <- data.frame(table(wellAndGrace$WaterLevelInFeetRelativeToNGVD29_NORM))
View(j)
wellAndGrace[which(wellAndGrace$WaterLevelInFeetRelativeToNGVD29_NORM=="-1.53572484833612"),]
wellAndGrace[which(wellAndGrace$WaterLevelInFeetRelativeToNGVD29_NORM=="-1.53572484833612"),]$SiteName
View(j)
wellAndGrace[which(wellAndGrace$WaterLevelInFeetRelativeToNGVD29_NORM==-1.53572484833612),]$SiteName
wellAndGrace[which(wellAndGrace$WaterLevelInFeetRelativeToNGVD29_NORM=="-1.53572484833612"),]$SiteName
wellAndGrace[which(wellAndGrace$WaterLevelInFeetRelativeToNGVD29_NORM=="-1.42244133038725"),]$SiteName
ggplot(data=wellAndGrace[which(wellAndGrace$SiteName=="007N007E33Q001M"),]) +
geom_point(aes(WaterLevelInFeetRelativeToNGVD29_NORM, GRACE_VALUE_NORM, color=SiteName))
ggplot(data=wellAndGrace[-which(wellAndGrace$SiteName=="007N007E33Q001M"),]) +
geom_point(aes(WaterLevelInFeetRelativeToNGVD29_NORM, GRACE_VALUE_NORM, color=SiteName))
linearModel <- lm(GRACE_VALUE_NORM~WaterLevelInFeetRelativeToNGVD29_NORM, data=wellAndGrace[-which(wellAndGrace$SiteName=="007N007E33Q001M"),])
summary(linearModel)
install.packages("elevatr")
library(elevatr)
data(lake)
x <- get_elev_raster(lake, z = 12)
library(rgdal)
install.packages("rgdal")
library(elevatr)
library(rgdal)
data(lake)
x <- get_elev_raster(lake, z = 12)
View(lake)
box <- data.frame(x=c(86.924975), y=c(27.988119))
x <- get_elev_raster(box, z = 12)
View(lake)
View(lake)
lake@proj4string
x <- get_elev_raster(box, z = 12, prj=lake@proj4string)
box <- data.frame(long=c(86.924975), lat=c(27.988119))
x <- get_elev_raster(box, z = 12)
box <- data.frame(x=c(86.924975), y=c(27.988119))
x <- get_elev_raster(box, z = 12)
loc_df <- data.frame(x = runif(6,min=sp::bbox(lake)[1,1],
max=sp::bbox(lake)[1,2]),
y = runif(6,min=sp::bbox(lake)[2,1],
max=sp::bbox(lake)[2,2]))
x <- get_elev_raster(locations = loc_df, prj = sp::proj4string(lake), z=10)
shiny::runApp('~/Documents/GitHub/marine-tardigrades')
